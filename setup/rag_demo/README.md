# ğŸ§ ğŸ“š RAG Demo â€“ AI Knowledge Library

In this mini project we turn a PDF into an **AI Knowledge Library** and let an
AI â€œLibrarianâ€ answer questions about it.

You will see:

1. PDF â†’ text â†’ **knowledge cards** (chunks)
2. Each card gets a **secret number code** (embedding)
3. The computer **finds the best cards** for your question (Retrieval)
4. We build an **augmented prompt** and ask the AI (Generation)
5. We save a **long-term memory log** of each question + answer

You only need to run one file: `rag_main.py`.

---

## 1. Folder structure

When you open the project in VS Code, you should see something like:

```text
rag_demo/
  â”œâ”€ rag_main.py                 # ğŸš€ main script you will run
  â”œâ”€ rag_utils/                  # helper modules used by main
  â”‚   â”œâ”€ __init__.py
  â”‚   â”œâ”€ rag_config.py           # paths, model names, constants
  â”‚   â”œâ”€ pdf_utils.py            # PDF â†’ pages â†’ knowledge cards
  â”‚   â”œâ”€ knowledge_library.py    # build & load AI Knowledge Library
  â”‚   â”œâ”€ retrieval.py            # find top-k best knowledge cards
  â”‚   â”œâ”€ prompt_utils.py         # build augmented messages from templates
  â”‚   â”œâ”€ rag_llm.py              # call the LLM and return JSON + usage
  â”‚   â””â”€ memory_store.py         # save long-term memory log
  â””â”€ data/
      â”œâ”€ data_sources/
      â”‚   â””â”€ book.pdf            # the PDF we will use
      â”œâ”€ knowledge_base/
      â”‚   â””â”€ knowledge_library.json   # created on first run
      â”œâ”€ outputs/
      â”‚   â””â”€ conversation_log.jsonl   # long-term memory (one JSON per line)
      â”œâ”€ prompts/
      â”‚   â”œâ”€ system_prompt.txt   # tells the AI its role
      â”‚   â””â”€ user_prompt.txt     # how we show snippets + question
      â””â”€ models/
          â””â”€ ... MiniLM files go here (see next section)
```
You will mostly touch:
- data/data_sources/book.pdf
- rag_main.py
 - the two prompt files in data/prompts/ (if you want to change how the AI talks)

## 2. MiniLM model (local copy from a zip)
We will use the MiniLM embedding model that you can download from hugging face or get from your team lead.
1. Get the model zip file from your team lead.
2. Unzip it into the `data/models/` folder inside this project.

When you finish, it should look like:

```text
rag_demo/
  â””â”€ data/
      â””â”€ models/
          â””â”€ models--sentence-transformers--all-MiniLM-L6-v2/

## 3. Running the demo

1. Open the `rag_demo` folder in **VS Code**.
2. Make sure the correct **Python environment** is selected (your teacher will
   have set this up).
3. Open the **terminal** in VS Code  
   (menu: **View â†’ Terminal**).
4. In the terminal, run:

   ```bash
   python rag_main.py
  ```
Youâ€™re now running the full RAG pipeline:
- Build or reuse the AI Knowledge Library
- Ask a question
- Retrieve the best knowledge cards
- Build an augmented prompt
- Call the language model
- Save the conversation into long-term memory
